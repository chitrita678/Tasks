import requests
from bs4 import BeautifulSoup
import csv

def scrape_website(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Error fetching data: {e}")
        return

    soup = BeautifulSoup(response.text, "html.parser")

    # Try scraping "quotes.toscrape.com" style pages
    quotes = soup.find_all("div", class_="quote")
    if not quotes:
        print("⚠️ Could not find 'quotes' on this site. Try another site.")
        return

    data = []
    print("\n=== Scraped Data ===\n")
    for i, quote in enumerate(quotes, start=1):
        text = quote.find("span", class_="text").get_text(strip=True)
        author = quote.find("small", class_="author").get_text(strip=True)
        tags = [tag.get_text(strip=True) for tag in quote.find_all("a", class_="tag")]

        # Console output
        print(f"{i}. {text}")
        print(f"   - {author}")
        print(f"   Tags: {', '.join(tags)}\n")

        # Add to list for CSV
        data.append([i, text, author, ", ".join(tags)])

    # Save to CSV file
    filename = "scraped_data.csv"
    try:
        with open(filename, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["ID", "Quote", "Author", "Tags"])
            writer.writerows(data)
        print(f"✅ Data saved to {filename}")
    except Exception as e:
        print(f"Error saving CSV: {e}")

def main():
    url = input("Enter website URL to scrape (example: http://quotes.toscrape.com): ").strip()
    scrape_website(url)

if __name__ == "__main__":
    main()
